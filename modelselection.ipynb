{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A | Parameter Estimation and Regularization\n",
    "### Spring 2024, Week 5\n",
    "Objectives:\n",
    "- gain intuition for parameter estimation strategy\n",
    "- gain intuition for cost function landscapes\n",
    "- contextualize MLR parameters (coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, Dropdown, Checkbox, HBox, VBox, Button, Output, Layout, IntSlider\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r week5_modelselection/\n",
    "! git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr24/week5_modelselection.git\n",
    "! cp -r week5_modelselection/* .\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "We have spoken about using a cost function like SSE for estimating the parameters (coefficients) of linear regression models. Today we will begin by looking at parameter estimation in the context of a different type of model.\n",
    "\n",
    "The [SIR model](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology) is an ODE model used to study the rate of spread of infectious diseases in a population. Its simplest form (excluding births, deaths, and spatial spread of a population) is as follows:\n",
    "\n",
    "$\\frac{dS}{dt} = -\\beta\\frac{SI}{N}$\n",
    "\n",
    "$\\frac{dI}{dt} = -\\beta \\frac{SI}{N} - \\gamma I$\n",
    "\n",
    "$\\frac{dR}{dt} = \\gamma I$\n",
    "\n",
    "Where:\n",
    "\n",
    "$S$ = susceptible individuals\n",
    "\n",
    "$I$ = infected individuals\n",
    "\n",
    "$R$ = recovered individuals\n",
    "\n",
    "$N$ = total population\n",
    "\n",
    "$\\beta$ = transmission rate\n",
    "\n",
    "$\\gamma$= recovery rate \n",
    "\n",
    "\n",
    "Lets say we had data that describes the number of infected individuals over time:\n",
    "\n",
    "![Infected Individuals](images/infected_individuals.png)\n",
    "\n",
    "\n",
    "Lets say we know the population size is 1000 and we know that initially one person was infected. So at time 0, the number of susceptible individuals was 999, the number of infected individuals was 1, and the number of recovered individuals was 0. How can we use these data to learn about the transmission rate and recovery rate of this disease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are synthetic data\n",
    "data_df = pd.read_csv('data/individuals_infected.csv')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIR model differential equations\n",
    "def sir_model(y, t, N, beta, gamma):\n",
    "    S, I, R = y\n",
    "    dSdt = -beta * S * I / N\n",
    "    dIdt = beta * S * I / N - gamma * I\n",
    "    dRdt = gamma * I\n",
    "    return dSdt, dIdt, dRdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumed data and initial conditions\n",
    "data = data_df['infected'].values\n",
    "t = data_df['time'].values\n",
    "N = 1000  # Total population\n",
    "I0 = 1  # Initial number of infected individuals\n",
    "S0 = N - I0  # Initial number of susceptible individuals\n",
    "R0 = 0  # Initial number of recovered individuals\n",
    "\n",
    "# Define the cost functions:\n",
    "# mean squared error:\n",
    "def mse(actual, predicted):\n",
    "    return np.mean((actual - predicted)**2)\n",
    "# sum of squared errors:\n",
    "def sse(actual, predicted):\n",
    "    return np.sum((actual - predicted)**2)\n",
    "# mean absolute error:\n",
    "def mae(actual, predicted):\n",
    "    return np.mean(np.abs(actual - predicted))\n",
    "# sum of absolute errors:\n",
    "def sae(actual, predicted):\n",
    "    return np.sum(np.abs(actual - predicted))\n",
    "# mean error\n",
    "def me(actual, predicted):\n",
    "    return np.mean(actual - predicted)\n",
    "# sum of errors\n",
    "def se(actual, predicted):\n",
    "    return np.sum(actual - predicted)\n",
    "\n",
    "\n",
    "# Function to perform parameter estimation\n",
    "def estimate_parameters(beta_range, gamma_range, num_samples, cost_func):\n",
    "    # Initialize the parameter space\n",
    "    beta_values = np.linspace(beta_range[0], beta_range[1], num_samples)\n",
    "    gamma_values = np.linspace(gamma_range[0], gamma_range[1], num_samples)\n",
    "\n",
    "    # Initialize the best cost and parameters\n",
    "    best_cost = float('inf')\n",
    "    best_params = None\n",
    "    best_fit = None\n",
    "    \n",
    "    # For every value in the parameter space\n",
    "    for beta in beta_values:\n",
    "        for gamma in gamma_values:\n",
    "            # Solve SIR model\n",
    "            ret = odeint(sir_model, [S0, I0, R0], t, args=(N, beta, gamma))\n",
    "            S, I, R = ret.T\n",
    "            \n",
    "            # Compute the cost\n",
    "            cost = cost_func(data, I)\n",
    "            # Keep track of lowest cost parameters\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_params = (beta, gamma)\n",
    "                best_fit = I\n",
    "    \n",
    "    # Plot the best fit\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(t, data, '.', label='Data')\n",
    "    plt.plot(t, best_fit, '-', label=f'Best SIR Fit using {cost_func.__name__.upper()}')\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel('Number of Infected Individuals')\n",
    "    plt.title(f'Best Fit SIR Model using {cost_func.__name__.upper()}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best parameters and cost\n",
    "    print(\"Best parameters: Beta =\", best_params[0], \", Gamma =\", best_params[1])\n",
    "    print(\"Best Cost (using\", cost_func.__name__.upper(), \"):\", best_cost)  \n",
    "    \n",
    "    return best_params, best_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above sets up a brute-force function you can use to estimate the SIR parameters that best fit these data. It takes a user specified range of potential $\\beta$ values, range of potential $\\gamma$ values, number of samples, and a cost function. In pseudocode, the funtion does the following:\n",
    "\n",
    "1. Creates a list of length num_samples of $\\beta$ values equally distributed in the user specified range of potential $\\beta$ values\n",
    "2. Creates a list of length num_samples of $\\gamma$ values equally distributed in the user specified range of potential $\\gamma$ values\n",
    "3. Solves the SIR ODEs once for each possible pairing of parameters in the $\\beta$ list and parameters in the $\\gamma$ list\n",
    "4. Calculates the cost of the model output with each parameter pairing\n",
    "5. Returns the parameters that yield the lowest model output cost\n",
    "6. Plots the SIR model predictions for number of infected individuals over time using the best found parameters over the actual data\n",
    "\n",
    "An example usage of the function is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "beta_range = (0.1, .2)  # range of beta to test\n",
    "gamma_range = (0.01, 0.02)  # range of gamma to test\n",
    "num_samples = 10  # number of values to sample in each range\n",
    "cost_function = mse  # You can change this to mse (mean of squared errors), sse (sum of squared errors), mae (mean of absolute errors), sae (sum of absolute errors), me (mean error), or se (sum of errors)\n",
    "\n",
    "best_params, best_cost = estimate_parameters(beta_range, gamma_range, num_samples, cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the parameter ranges, the number of samples, and the cost function above to try to get the lowest cost you can. Find parameter search spaces (ranges for $\\beta$ and $\\gamma$) and a number of samples that gives you a good fit and then answer the following discussion questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCUSSION QUESTIONS:\n",
    "- What are the lowest scoring parameters you can find?\n",
    "- How does the best fit set of parameters found using me and se as cost functions compare to the best fit set of parameters found using mae and sae?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT QUESTIONS:\n",
    "- please complete questions 10 and 11 in the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Cost Function Landscapes\n",
    "\n",
    "A cost function landscape represents the values of a cost function over a range of parameter settings. In parameter estimation, our goal is often to find the parameter values that minimize this cost function. The shape of the landscape can greatly affect the ease and reliability of finding the global minimum. Convex landscapes, where any line segment between two points on the surface does not dip below the surface, ensure that any local minimum is also a global minimum. The plot below shows a simple convex function. Notice how any two points on the curve, when connected by a straight line, always stay above the curve. You can move the line around using the sliders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a convex function and a line segment between two points\n",
    "def plot_convex(a, b):\n",
    "    x = np.linspace(-10, 10, 400)\n",
    "    y = x**2\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, label='$f(x) = x^2$')\n",
    "    \n",
    "    # Points and line segment\n",
    "    ya = a**2\n",
    "    yb = b**2\n",
    "    plt.plot([a, b], [ya, yb], 'ro-')\n",
    "    \n",
    "    plt.title('Interactive Convex Function: $f(x) = x^2$')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Widgets for the points a and b\n",
    "a_slider = widgets.FloatSlider(value=-5, min=-10, max=10, step=0.1, description='Point a:')\n",
    "b_slider = widgets.FloatSlider(value=5, min=-10, max=10, step=0.1, description='Point b:')\n",
    "\n",
    "# Display the widgets and output\n",
    "widgets.interactive(plot_convex, a=a_slider, b=b_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows a non-convex function, $f(x) = x^3 - 3x$. You can see that the function has multiple local minima and maxima. A line segment connecting points on different slopes of a local minimum dips below the curve, illustrating non-convex behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a non-convex function and a line segment between two points\n",
    "def plot_non_convex(a, b):\n",
    "    x = np.linspace(-3, 3, 400)\n",
    "    y = x**3 - 3*x\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, label='$f(x) = x^3 - 3x$')\n",
    "    \n",
    "    # Points and line segment\n",
    "    ya = a**3 - 3*a\n",
    "    yb = b**3 - 3*b\n",
    "    plt.plot([a, b], [ya, yb], 'ro-')\n",
    "    \n",
    "    plt.title('Interactive Non-Convex Function: $f(x) = x^3 - 3x$')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Widgets for the points a and b\n",
    "a_slider_nonconvex = widgets.FloatSlider(value=-2, min=-3, max=3, step=0.1, description='Point a:')\n",
    "b_slider_nonconvex = widgets.FloatSlider(value=2, min=-3, max=3, step=0.1, description='Point b:')\n",
    "\n",
    "# Display the widgets and output\n",
    "widgets.interactive(plot_non_convex, a=a_slider_nonconvex, b=b_slider_nonconvex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of parameter estimation, a convex cost function landscape is ideal because it guarantees that any local minimum found is also the global minimum. It ensures that the optimization process will not get trapped in a local minimum that is not the best possible solution.\n",
    "\n",
    "In this exercise we have been conceptualizing parameter estimation as a process of trying a bunch of different parameters and choosing the parameters with the lowest cost. However, in many cases if we know the cost function is convex, we instead analytically solve for the parameter values that set the derivative of the cost function to 0. If the cost function is convex, this will always be the global minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT QUESTIONS:\n",
    "- please complete questions 12, 13, and 14 in the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression Revisited\n",
    "\n",
    "Just as in the SIR mdoel where we estimate parameters like infection and recovery rates, in multiple linear regression (MLR), we estimate coefficients that multiply the predictor (independent) variables. These coefficients determine how much each predictor (independent) affects the response (dependent) variable. The challenge is to determine the set of coefficients that best fit the observed data, typically by minimizing a cost function, such as the sum of squared errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges of Large or Small Coefficients in MLR\n",
    "\n",
    "In multiple linear regression, encountering coefficients that are very large or very small can indicate issues such as multicollinearity or that certain predictors have minimal influence on the response variable. Large coefficients can lead to model instability, where small changes in data lead to large changes in parameter estimates, while very small coefficients might suggest that some predictors are not contributing much to the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We will again be working on real breast cancer data from the [Wisconsin Diagnostic Breast Cancer Database (WDBC)](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). We will replace the \"malignant\" and \"benign\" values in the 'Diagnosis' column with 0s and 1s. A 0 will indicate malignant and a 1 will indicate benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "features = cancer.feature_names\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data=X, columns=features)\n",
    "df['Diagnosis'] = y  # 0 for malignant, 1 for benign\n",
    "\n",
    "features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'Diagnosis']\n",
    "\n",
    "df = df[features]\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remind ourselves of the distribution of these features for cancerous vs non cancerous cells, lets plot a correlation plot for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation plots of a selection of independent variables against the dependent variable\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(15, 10))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "fig.suptitle('Correlation Plots')\n",
    "\n",
    "selected_features = features[:10]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(selected_features):\n",
    "        ax.scatter(df[selected_features[i]], df['Diagnosis'])\n",
    "        ax.set_title(f'{selected_features[i]} vs Diagnosis')\n",
    "        ax.set_xlabel(selected_features[i])\n",
    "        ax.set_ylabel('Diagnosis')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below normalizes the data and performs MLR with all selected features as independent variables and diagnosis as the dependent variable. Select different features (hold shift or command while clicking to select multiple features) to see how the coefficients estimated by MLR. Note the R-squared value printed below the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to update the model based on selected features\n",
    "def update_model(selected_features):\n",
    "    # Normalize the data for the selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df[list(selected_features)])\n",
    "\n",
    "    # Initialize models\n",
    "    mlr = LinearRegression()\n",
    "\n",
    "    # Fit models\n",
    "    mlr.fit(X_scaled, y)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_mlr = mlr.predict(X_scaled)\n",
    "\n",
    "    # Coefficients\n",
    "    mlr_coefs = pd.Series(mlr.coef_, index=selected_features)\n",
    "\n",
    "    # R-squared values\n",
    "    r2_mlr = r2_score(y, y_pred_mlr)\n",
    "\n",
    "    # Display the coefficients and R-squared values\n",
    "    coefs_df = pd.DataFrame({\n",
    "        'MLR Coefficients': mlr_coefs,\n",
    "    })\n",
    "    display(pd.concat([coefs_df]))\n",
    "    print(f\"MLR R-squared: {r2_mlr:.2f}\")\n",
    "\n",
    "# Create widget for selecting features\n",
    "select_features = widgets.SelectMultiple(\n",
    "    options=features[:10],\n",
    "    value=[features[0]],\n",
    "    description='Features',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Interactive widget\n",
    "widgets.interactive(update_model, selected_features=select_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression may zero out some coefficients, suggesting these features are less important for predicting the outcome, whereas Ridge tends to reduce the magnitude of coefficients more uniformly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCUSSION QUESTION:\n",
    "- Why is the greatest MLR coefficient in the model using all the features different from the feature with the greatest predictive capacity?\n",
    "- Several parameters can be dropped in isolation and retain the same predictive capacity. Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT QUESTIONS:\n",
    "- Please compete question 15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
